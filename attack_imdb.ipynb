{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack IMDB model\n",
    "\n",
    "The IMDB dataset contains movie reviews that are labeled either positive or negative. Each review is a paragraph consists of multiple sentences.\n",
    "\n",
    "Here, we attempt to attack a **wordCNN** model for IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from neural_networks import word_cnn, char_cnn, bd_lstm, lstm\n",
    "import os\n",
    "from read_files import split_imdb_files, split_yahoo_files, split_agnews_files\n",
    "from word_level_process import word_process, get_tokenizer, text_to_vector_for_all\n",
    "from config import config\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "dataset = \"imdb\"\n",
    "model_name = \"pretrained_word_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# stanfordnlp.download('en')\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word_cnn model...\n",
      "successfully load model\n"
     ]
    }
   ],
   "source": [
    "model = word_cnn(dataset)\n",
    "model_path = r'./runs/{}/{}.dat'.format(dataset, model_name)\n",
    "model.load_weights(model_path)\n",
    "print(\"successfully load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IMDB dataset\n",
      "successfully load data\n"
     ]
    }
   ],
   "source": [
    "# Data label:\n",
    "# [1 0] is negative review\n",
    "# [0 1] is positive review\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels = split_imdb_files()\n",
    "x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)\n",
    "print('successfully load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_str` allows a string representation of movie review be predicted without manually converting to sequence first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a string input with a model directly\n",
    "# so skips the need of converting to sequence first...\n",
    "def predict_str(model, s):\n",
    "    maxlen = config.word_max_len[dataset]\n",
    "    tokenizer = get_tokenizer(dataset)\n",
    "    s_seq = tokenizer.texts_to_sequences([s])\n",
    "    s_seq = sequence.pad_sequences(s_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    s_sep = s_seq[0]\n",
    "    return model.predict(s_seq)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sample one sentence from entire testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "\n",
      "model predict  [0.92145205 0.07800014]\n",
      "predict with predict_str  [0.92145205 0.07800014]\n",
      "true label  [1 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# idx = random.randint(0, x_test.shape[0] - 1)\n",
    "idx = 15557\n",
    "\n",
    "xi = x_test[idx:idx+1]\n",
    "yi = y_test[idx:idx+1][0]\n",
    "xi_text = test_texts[idx]\n",
    "\n",
    "print(xi_text)\n",
    "print()\n",
    "print(\"model predict \", model.predict(xi)[0])\n",
    "print(\"predict with predict_str \", predict_str(model, xi_text))\n",
    "print(\"true label \", yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks a review to sentences based on `StanfordParser`'s result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_list(doc):\n",
    "    sentences = []\n",
    "    for words in doc.sentences:\n",
    "        sentence = words.words[0].text\n",
    "        for word in words.words[1:]:\n",
    "            if word.upos != 'PUNCT' and not word.text.startswith('\\''):\n",
    "                sentence += ' '\n",
    "            sentence += word.text\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paraneter of a StanfordNLP doc objectï¼›\n",
    "'_text', '_conll_file', '_sentences'\n",
    "\n",
    "Parameters of doc.conll_file:\n",
    "'ignore_gapping', '_file', '_from_str', '_sents', '_num_words'\n",
    "\n",
    "Parameters of doc.sentence object:\n",
    "'_tokens', '_words', '_dependencies'\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(xi_text)\n",
    "sentences = sentence_list(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute **Sentence Saliency**.\n",
    "\n",
    "Let $x = s_1s_2\\dots s_n$ be a input consists of $n$ sentences. Let $y$ be $x$'s true label. The sentence saliency for sentence $s_k$ is:\n",
    "\n",
    "$$S(y|s_k) = P(y|x) - P(x|s_1s_2\\dots s_{k-1}s_{k+1}\\dots s_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_saliency(model, sentences, label):\n",
    "    true_pred = predict_str(model, ' '.join(sentences))\n",
    "    if label[0] == 1:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "    scores = []\n",
    "    for i in range(len(sentences)):\n",
    "        x_hat = ' '.join(sentences[0:i] + sentences[i+1:])\n",
    "        scores.append(true_pred[idx] - predict_str(model, x_hat)[idx])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_scores = sentence_saliency(model, sentences, yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05681753, 0.0052631497, -0.028491437, -0.002485156, 0.03154117, -0.022087038, -0.029951096, 0.15772057]\n"
     ]
    }
   ],
   "source": [
    "print(saliency_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
