{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack IMDB model\n",
    "\n",
    "The IMDB dataset contains movie reviews that are labeled either positive or negative. Each review is a paragraph consists of multiple sentences.\n",
    "\n",
    "Here, we attempt to attack a **wordCNN** model for IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks import word_cnn, char_cnn, bd_lstm, lstm\n",
    "import os\n",
    "from read_files import split_imdb_files, split_yahoo_files, split_agnews_files\n",
    "from word_level_process import word_process, get_tokenizer, text_to_vector_for_all\n",
    "from config import config\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "dataset = \"imdb\"\n",
    "model_name = \"pretrained_word_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# stanfordnlp.download('en')\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word_cnn model...\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "successfully load model\n"
     ]
    }
   ],
   "source": [
    "model = word_cnn(dataset)\n",
    "model_path = r'./runs/{}/{}.dat'.format(dataset, model_name)\n",
    "model.load_weights(model_path)\n",
    "print(\"successfully load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IMDB dataset\n",
      "successfully load data\n"
     ]
    }
   ],
   "source": [
    "# Data label:\n",
    "# [1 0] is negative review\n",
    "# [0 1] is positive review\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels = split_imdb_files()\n",
    "x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)\n",
    "print('successfully load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_str` allows a string representation of movie review be predicted without manually converting to sequence first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a string input with a model directly\n",
    "# so skips the need of converting to sequence first...\n",
    "def predict_str(model, s):\n",
    "    maxlen = config.word_max_len[dataset]\n",
    "    tokenizer = get_tokenizer(dataset)\n",
    "    s_seq = tokenizer.texts_to_sequences([s])\n",
    "    s_seq = sequence.pad_sequences(s_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    s_sep = s_seq[0]\n",
    "    return model.predict(s_seq)[0]\n",
    "\n",
    "def predict_sentences(model, sentences):\n",
    "    return predict_str(model, ' '.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sample one sentence from entire testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "\n",
      "model predict  [0.92145205 0.07800014]\n",
      "predict with predict_str  [0.92145205 0.07800014]\n",
      "true label  [1 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# idx = random.randint(0, x_test.shape[0] - 1)\n",
    "idx = 15557\n",
    "\n",
    "xi = x_test[idx:idx+1]\n",
    "yi = y_test[idx:idx+1][0]\n",
    "xi_text = test_texts[idx]\n",
    "\n",
    "print(xi_text)\n",
    "print()\n",
    "print(\"model predict \", model.predict(xi)[0])\n",
    "print(\"predict with predict_str \", predict_str(model, xi_text))\n",
    "print(\"true label \", yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks a review to sentences based on `StanfordParser`'s result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_list(doc):\n",
    "    sentences = []\n",
    "    for words in doc.sentences:\n",
    "        sentence = words.words[0].text\n",
    "        for word in words.words[1:]:\n",
    "            if word.upos != 'PUNCT' and not word.text.startswith('\\''):\n",
    "                sentence += ' '\n",
    "            sentence += word.text\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paraneter of a StanfordNLP doc object；\n",
    "'_text', '_conll_file', '_sentences'\n",
    "\n",
    "Parameters of doc.conll_file:\n",
    "'ignore_gapping', '_file', '_from_str', '_sents', '_num_words'\n",
    "\n",
    "Parameters of doc.sentence object:\n",
    "'_tokens', '_words', '_dependencies'\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(xi_text)\n",
    "sentences = sentence_list(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute **Sentence Saliency**.\n",
    "\n",
    "Let $x = s_1s_2\\dots s_n$ be a input consists of $n$ sentences. Let $y$ be $x$'s true label. The sentence saliency for sentence $s_k$ is:\n",
    "\n",
    "$$S(y|s_k) = P(y|x) - P(x|s_1s_2\\dots s_{k-1}s_{k+1}\\dots s_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_saliency(model, sentences, label):\n",
    "    true_pred = predict_str(model, ' '.join(sentences))\n",
    "    if label[0] == 1:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "    scores = []\n",
    "    for i in range(len(sentences)):\n",
    "        x_hat = ' '.join(sentences[0:i] + sentences[i+1:])\n",
    "        scores.append(true_pred[idx] - predict_str(model, x_hat)[idx])\n",
    "    \n",
    "    return np.array(scores)\n",
    "\n",
    "def softmax(x, determinism = 21):\n",
    "    softmax = np.exp(np.multiply(determinism, x))\n",
    "    softmax /= np.sum(softmax)\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09044634 0.03063413 0.01507848 0.02603392 0.0531944  0.01724911\n",
      " 0.01462329 0.7527404 ]\n"
     ]
    }
   ],
   "source": [
    "saliency_scores = softmax(sentence_saliency(model, sentences, yi))\n",
    "print(saliency_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie.\n",
      "If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "I caught it again on television and it was much funnier.\n",
      "If you've nothing better to do and it's free on t.v. you could do worse.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(np.random.choice(sentences, p=saliency_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence paraphrasing\n",
    "\n",
    "**back translation**: translate input sentence to another language, then translate back to the original language. This is a technique commonly used for evaluation of language translations. Here, we use this technique to quickly generate a rephrase of original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n"
     ]
    }
   ],
   "source": [
    "print(xi_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test google cloud authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/weifanjiang/Documents/Personal/My Project-1e7426894fe6.json\"\n",
    "\n",
    "def implicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "implicit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans (af)\n",
      "Albanian (sq)\n",
      "Amharic (am)\n",
      "Arabic (ar)\n",
      "Armenian (hy)\n",
      "Azerbaijani (az)\n",
      "Basque (eu)\n",
      "Belarusian (be)\n",
      "Bengali (bn)\n",
      "Bosnian (bs)\n",
      "Bulgarian (bg)\n",
      "Catalan (ca)\n",
      "Cebuano (ceb)\n",
      "Chichewa (ny)\n",
      "Chinese (Simplified) (zh)\n",
      "Chinese (Traditional) (zh-TW)\n",
      "Corsican (co)\n",
      "Croatian (hr)\n",
      "Czech (cs)\n",
      "Danish (da)\n",
      "Dutch (nl)\n",
      "English (en)\n",
      "Esperanto (eo)\n",
      "Estonian (et)\n",
      "Filipino (tl)\n",
      "Finnish (fi)\n",
      "French (fr)\n",
      "Frisian (fy)\n",
      "Galician (gl)\n",
      "Georgian (ka)\n",
      "German (de)\n",
      "Greek (el)\n",
      "Gujarati (gu)\n",
      "Haitian Creole (ht)\n",
      "Hausa (ha)\n",
      "Hawaiian (haw)\n",
      "Hebrew (iw)\n",
      "Hindi (hi)\n",
      "Hmong (hmn)\n",
      "Hungarian (hu)\n",
      "Icelandic (is)\n",
      "Igbo (ig)\n",
      "Indonesian (id)\n",
      "Irish (ga)\n",
      "Italian (it)\n",
      "Japanese (ja)\n",
      "Javanese (jw)\n",
      "Kannada (kn)\n",
      "Kazakh (kk)\n",
      "Khmer (km)\n",
      "Kinyarwanda (rw)\n",
      "Korean (ko)\n",
      "Kurdish (Kurmanji) (ku)\n",
      "Kyrgyz (ky)\n",
      "Lao (lo)\n",
      "Latin (la)\n",
      "Latvian (lv)\n",
      "Lithuanian (lt)\n",
      "Luxembourgish (lb)\n",
      "Macedonian (mk)\n",
      "Malagasy (mg)\n",
      "Malay (ms)\n",
      "Malayalam (ml)\n",
      "Maltese (mt)\n",
      "Maori (mi)\n",
      "Marathi (mr)\n",
      "Mongolian (mn)\n",
      "Myanmar (Burmese) (my)\n",
      "Nepali (ne)\n",
      "Norwegian (no)\n",
      "Odia (Oriya) (or)\n",
      "Pashto (ps)\n",
      "Persian (fa)\n",
      "Polish (pl)\n",
      "Portuguese (pt)\n",
      "Punjabi (pa)\n",
      "Romanian (ro)\n",
      "Russian (ru)\n",
      "Samoan (sm)\n",
      "Scots Gaelic (gd)\n",
      "Serbian (sr)\n",
      "Sesotho (st)\n",
      "Shona (sn)\n",
      "Sindhi (sd)\n",
      "Sinhala (si)\n",
      "Slovak (sk)\n",
      "Slovenian (sl)\n",
      "Somali (so)\n",
      "Spanish (es)\n",
      "Sundanese (su)\n",
      "Swahili (sw)\n",
      "Swedish (sv)\n",
      "Tajik (tg)\n",
      "Tamil (ta)\n",
      "Tatar (tt)\n",
      "Telugu (te)\n",
      "Thai (th)\n",
      "Turkish (tr)\n",
      "Turkmen (tk)\n",
      "Ukrainian (uk)\n",
      "Urdu (ur)\n",
      "Uyghur (ug)\n",
      "Uzbek (uz)\n",
      "Vietnamese (vi)\n",
      "Welsh (cy)\n",
      "Xhosa (xh)\n",
      "Yiddish (yi)\n",
      "Yoruba (yo)\n",
      "Zulu (zu)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "translate_client = translate.Client()\n",
    "\n",
    "results = translate_client.get_languages()\n",
    "\n",
    "for language in results:\n",
    "    print(u'{name} ({language})'.format(**language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/weifanjiang/Documents/Personal/My Project-1e7426894fe6.json\"\n",
    "\n",
    "def back_translation(s_in, language='ko', show_mid=False):\n",
    "    translate_client = translate.Client()\n",
    "    mid_result = translate_client.translate(s_in, target_language=language)['translatedText']\n",
    "    if show_mid:\n",
    "        print(mid_result.replace(\"&#39;\", \"\\'\"))\n",
    "    en_result = translate_client.translate(mid_result, target_language=\"en\")['translatedText']\n",
    "    return en_result.replace(\"&#39;\", \"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "这是一部愚蠢的电影。当我在电影院放映时，一半以上的观众离开了一半。我一直坚持到最后。为了显示毅力？我又在电视上看到了，这很有趣。仍然绝不算是经典，甚至始终不是一成不变的搞笑，但是我的家庭有点长。我还是爱杰西卡·伦迪。如果您无事可做，而且电视上是免费的，那您可能会做得更糟。\n",
      "This is a stupid movie. When I screened in the cinema, more than half of the audience left. I stayed till the end. To show perseverance? I saw it on TV again and it was fun. It's still not a classic, it's not always funny, but my family is a bit long. I still love Jessica Lundy. If you have nothing to do and it's free on TV, you could do worse.\n"
     ]
    }
   ],
   "source": [
    "print(xi_text)\n",
    "print(back_translation(xi_text, language='zh', show_mid = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing sentence-level genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`perturb`: randomly select a sentence from the input paragraph with probability propotional to the saliency score of each sentence. Then apply rephrasing to the selected sentence. Return the paragraph with one modified sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache():\n",
    "    cache_file = \"google_translate_cache.pickle\"\n",
    "    if os.path.isfile(cache_file):\n",
    "        with open(cache_file, \"rb\") as f_in:\n",
    "            cache = pickle.load(f_in)\n",
    "    else:\n",
    "        cache = dict()\n",
    "    return cache\n",
    "\n",
    "def save_cache(cache):\n",
    "    cache_file = \"google_translate_cache.pickle\"\n",
    "    with open(cache_file, \"wb\") as f_out:\n",
    "        pickle.dump(cache, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(sentences, saliencies, cache, not_choose=set()):\n",
    "    \n",
    "    choices = list()\n",
    "    weights = list()\n",
    "    for i in range(len(sentences)):\n",
    "        if not i in not_choose:\n",
    "            choices.append(sentences[i])\n",
    "            weights.append(saliencies[i])\n",
    "    weights = softmax(weights)\n",
    "    \n",
    "    choice = np.random.choice(choices, p=weights)\n",
    "    \n",
    "    chosen_language = set()\n",
    "    all_languages = ['es', 'ko', 'zh', 'th']\n",
    "    rephrase = choice\n",
    "    while (len(chosen_language) < len(all_languages)) and (rephrase == choice):\n",
    "        language = np.random.choice(all_languages)\n",
    "        while language in chosen_language:\n",
    "            language = np.random.choice(all_languages)\n",
    "        chosen_language.add(language)\n",
    "        rephrase = cache.get((choice, language), None)\n",
    "        if rephrase is None:\n",
    "            rephrase = back_translation(choice, language=language)\n",
    "            cache[(choice, language)] = rephrase\n",
    "    new_paragraph = []\n",
    "    for j, sen in enumerate(sentences):\n",
    "        if sen == choice:\n",
    "            new_paragraph.append(rephrase)\n",
    "            changed_idx = j\n",
    "        else:\n",
    "            new_paragraph.append(sen)\n",
    "    return new_paragraph, changed_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crossover`: randomly produce a new paragraph by combining two input paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(sentences1, sentences2, p1_changed, p2_changed):\n",
    "    child = list()\n",
    "    child_idx = set()\n",
    "    for i in range(len(sentences1)):\n",
    "        if random.randint(0,1) == 0:\n",
    "            if i in p1_changed:\n",
    "                child_idx.add(i)\n",
    "            child.append(sentences1[i])\n",
    "        else:\n",
    "            if i in p2_changed:\n",
    "                child_idx.add(i)\n",
    "            child.append(sentences2[i])\n",
    "    return child, child_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`genetic`: main function to perform genetic attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic(x0, y0, model, population, generation):\n",
    "    \n",
    "    if y0[0] == 0:\n",
    "        y_adv = [1, 0]\n",
    "    else:\n",
    "        y_adv = [0, 1]\n",
    "    \n",
    "    cache = load_cache()\n",
    "    \n",
    "    doc = nlp(xi_text)\n",
    "    sentences = sentence_list(doc)\n",
    "    saliency_scores = sentence_saliency(model, sentences, yi)\n",
    "    \n",
    "    prob_0 = predict_str(model, x0)\n",
    "    \n",
    "    print(\"clean sample's prediction: {}\".format(prob_0))\n",
    "    \n",
    "    # We want target_idx of adv.example's prediction to be larger\n",
    "    # than 0.5\n",
    "    target_idx = np.argmin(prob_0)\n",
    "    print('target is to make index {} > 0.5'.format(target_idx))\n",
    "    \n",
    "    gen0 = list()\n",
    "    chosen_idx = dict()\n",
    "    for i in range(population):\n",
    "        chosen = set()\n",
    "        sample, idx = perturb(sentences, saliency_scores, cache)\n",
    "        gen0.append(sample)\n",
    "        chosen.add(idx)\n",
    "        chosen_idx[i] = chosen\n",
    "    \n",
    "    curr_gen = gen0\n",
    "    for i in range(generation):\n",
    "        \n",
    "        print('generation {}'.format(i + 1))\n",
    "        \n",
    "        sample_weight = list()\n",
    "        for j, sample in enumerate(curr_gen):\n",
    "            sample_pred = predict_sentences(model, sample)\n",
    "            print(\"population {} pred: {}\".format(j, sample_pred))\n",
    "            if sample_pred[target_idx] > 0.5:\n",
    "                print('successful adv. example found!')\n",
    "                save_cache(cache)\n",
    "                return ' '.join(sample)\n",
    "            else:\n",
    "                sample_weight.append(sample_pred[target_idx])\n",
    "        sample_weight = softmax(np.array(sample_weight))\n",
    "        print('population with fitness scores: {}'.format(sample_weight))\n",
    "        \n",
    "        next_gen = list()\n",
    "        next_chosen = dict()\n",
    "        for j in range(population):\n",
    "            idx_list = list(range(population))\n",
    "            p1 = np.random.choice(idx_list, p=sample_weight)\n",
    "            p2 = np.random.choice(idx_list, p=sample_weight)\n",
    "            print(\"child {} generated with parents {} and {}\".format(j, p1, p2))\n",
    "            child, child_change = crossover(curr_gen[p1], curr_gen[p2], chosen_idx[p1], chosen_idx[p2])\n",
    "            saliency_scores = sentence_saliency(model, child, y0)\n",
    "            child_mutate, change_idx = perturb(sentences, saliency_scores, cache)\n",
    "            next_gen.append(child_mutate)\n",
    "            child_change.add(change_idx)\n",
    "            next_chosen[j] = child_change\n",
    "        curr_gen = next_gen\n",
    "        chosen_idx = next_chosen\n",
    "\n",
    "    save_cache(cache)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sample's prediction: [0.92145205 0.07800014]\n",
      "target is to make index 1 > 0.5\n",
      "generation 1\n",
      "population 0 pred: [0.8864471  0.11223546]\n",
      "population 1 pred: [0.82256144 0.1749145 ]\n",
      "population 2 pred: [0.8231986  0.17350492]\n",
      "population with fitness scores: [0.11975881 0.44663414 0.43360704]\n",
      "child 0 generated with parents 2 and 1\n",
      "child 1 generated with parents 1 and 1\n",
      "child 2 generated with parents 2 and 1\n",
      "generation 2\n",
      "population 0 pred: [0.7827212  0.21101502]\n",
      "population 1 pred: [0.80019045 0.19863959]\n",
      "population 2 pred: [0.80019045 0.19863959]\n",
      "population with fitness scores: [0.39334738 0.30332634 0.30332634]\n",
      "child 0 generated with parents 0 and 0\n",
      "child 1 generated with parents 0 and 1\n",
      "child 2 generated with parents 2 and 0\n",
      "generation 3\n",
      "population 0 pred: [0.82256144 0.1749145 ]\n",
      "population 1 pred: [0.82256144 0.1749145 ]\n",
      "population 2 pred: [0.8231986  0.17350492]\n",
      "population with fitness scores: [0.33660594 0.33660594 0.32678807]\n",
      "child 0 generated with parents 0 and 0\n",
      "child 1 generated with parents 2 and 0\n",
      "child 2 generated with parents 2 and 0\n",
      "generation 4\n",
      "population 0 pred: [0.92145205 0.07800014]\n",
      "population 1 pred: [0.7827212  0.21101502]\n",
      "population 2 pred: [0.92145205 0.07800014]\n",
      "population with fitness scores: [0.05454038 0.89091927 0.05454038]\n",
      "child 0 generated with parents 1 and 1\n",
      "child 1 generated with parents 1 and 1\n",
      "child 2 generated with parents 1 and 1\n",
      "generation 5\n",
      "population 0 pred: [0.9083737  0.09020738]\n",
      "population 1 pred: [0.8231986  0.17350492]\n",
      "population 2 pred: [0.82256144 0.1749145 ]\n",
      "population with fitness scores: [0.07890594 0.45373118 0.46736285]\n",
      "child 0 generated with parents 2 and 2\n",
      "child 1 generated with parents 2 and 1\n",
      "child 2 generated with parents 0 and 2\n",
      "generation 6\n",
      "population 0 pred: [0.91883844 0.08032496]\n",
      "population 1 pred: [0.7827212  0.21101502]\n",
      "population 2 pred: [0.92145205 0.07800014]\n",
      "population with fitness scores: [0.05711333 0.88849473 0.05439195]\n",
      "child 0 generated with parents 1 and 1\n",
      "child 1 generated with parents 1 and 1\n",
      "child 2 generated with parents 1 and 1\n",
      "generation 7\n",
      "population 0 pred: [0.9083737  0.09020738]\n",
      "population 1 pred: [0.82256144 0.1749145 ]\n",
      "population 2 pred: [0.9083737  0.09020738]\n",
      "population with fitness scores: [0.12621422 0.7475716  0.12621422]\n",
      "child 0 generated with parents 1 and 1\n",
      "child 1 generated with parents 2 and 1\n",
      "child 2 generated with parents 1 and 2\n",
      "generation 8\n",
      "population 0 pred: [0.8464523  0.15179555]\n",
      "population 1 pred: [0.9083737  0.09020738]\n",
      "population 2 pred: [0.88914716 0.1109354 ]\n",
      "population with fitness scores: [0.58881295 0.16154069 0.24964641]\n",
      "child 0 generated with parents 2 and 0\n",
      "child 1 generated with parents 2 and 0\n",
      "child 2 generated with parents 0 and 0\n"
     ]
    }
   ],
   "source": [
    "genetic(xi_text, yi, model, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Still by no means a classic, or even consistently hilarious but the family kinda grew on me.', 'zh'): \"It's still not a classic, it's not always funny, but my family is a bit long.\", ('When I saw it in a movie theater more than half the audience left before it was half over.', 'zh'): 'When I screened in the cinema, more than half of the audience left.', ('I caught it again on television and it was much funnier.', 'th'): 'I caught it again on television and it was much more fun.', ('I stayed to the bitter end.', 'zh'): 'I stayed till the end.', ('To show fortitude?', 'ko'): 'To show courage?', (\"If you've nothing better to do and it's free on t.v. you could do worse.\", 'th'): 'If you have nothing to do better, and free on TV', (\"If you've nothing better to do and it's free on t.v. you could do worse.\", 'zh'): \"If you have nothing to do and it's free on TV, you could do worse.\", ('This is a stupid movie.', 'th'): 'This is a stupid movie', ('I stayed to the bitter end.', 'th'): \"I'm at the bitter end\", ('This is a stupid movie.', 'es'): 'This is a stupid movie.', ('This is a stupid movie.', 'zh'): 'This is a stupid movie.', ('This is a stupid movie.', 'ko'): 'This is a silly movie.'}\n"
     ]
    }
   ],
   "source": [
    "cache = load_cache()\n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
