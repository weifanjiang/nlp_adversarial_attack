{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack IMDB model\n",
    "\n",
    "The IMDB dataset contains movie reviews that are labeled either positive or negative. Each review is a paragraph consists of multiple sentences.\n",
    "\n",
    "Here, we attempt to attack a **wordCNN** model for IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks import word_cnn, char_cnn, bd_lstm, lstm\n",
    "import os\n",
    "from read_files import split_imdb_files, split_yahoo_files, split_agnews_files\n",
    "from word_level_process import word_process, get_tokenizer, text_to_vector_for_all\n",
    "from config import config\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "dataset = \"imdb\"\n",
    "model_name = \"pretrained_word_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/weifanjiang/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# stanfordnlp.download('en')\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word_cnn model...\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "successfully load model\n"
     ]
    }
   ],
   "source": [
    "model = word_cnn(dataset)\n",
    "model_path = r'./runs/{}/{}.dat'.format(dataset, model_name)\n",
    "model.load_weights(model_path)\n",
    "print(\"successfully load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IMDB dataset\n",
      "successfully load data\n"
     ]
    }
   ],
   "source": [
    "# Data label:\n",
    "# [1 0] is negative review\n",
    "# [0 1] is positive review\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels = split_imdb_files()\n",
    "x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)\n",
    "print('successfully load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_str` allows a string representation of movie review be predicted without manually converting to sequence first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a string input with a model directly\n",
    "# so skips the need of converting to sequence first...\n",
    "def predict_str(model, s):\n",
    "    maxlen = config.word_max_len[dataset]\n",
    "    tokenizer = get_tokenizer(dataset)\n",
    "    s_seq = tokenizer.texts_to_sequences([s])\n",
    "    s_seq = sequence.pad_sequences(s_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    s_sep = s_seq[0]\n",
    "    return model.predict(s_seq)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sample one sentence from entire testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "\n",
      "model predict  [0.92145205 0.07800014]\n",
      "predict with predict_str  [0.92145205 0.07800014]\n",
      "true label  [1 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# idx = random.randint(0, x_test.shape[0] - 1)\n",
    "idx = 15557\n",
    "\n",
    "xi = x_test[idx:idx+1]\n",
    "yi = y_test[idx:idx+1][0]\n",
    "xi_text = test_texts[idx]\n",
    "\n",
    "print(xi_text)\n",
    "print()\n",
    "print(\"model predict \", model.predict(xi)[0])\n",
    "print(\"predict with predict_str \", predict_str(model, xi_text))\n",
    "print(\"true label \", yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks a review to sentences based on `StanfordParser`'s result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_list(doc):\n",
    "    sentences = []\n",
    "    for words in doc.sentences:\n",
    "        sentence = words.words[0].text\n",
    "        for word in words.words[1:]:\n",
    "            if word.upos != 'PUNCT' and not word.text.startswith('\\''):\n",
    "                sentence += ' '\n",
    "            sentence += word.text\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paraneter of a StanfordNLP doc object；\n",
    "'_text', '_conll_file', '_sentences'\n",
    "\n",
    "Parameters of doc.conll_file:\n",
    "'ignore_gapping', '_file', '_from_str', '_sents', '_num_words'\n",
    "\n",
    "Parameters of doc.sentence object:\n",
    "'_tokens', '_words', '_dependencies'\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(xi_text)\n",
    "sentences = sentence_list(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute **Sentence Saliency**.\n",
    "\n",
    "Let $x = s_1s_2\\dots s_n$ be a input consists of $n$ sentences. Let $y$ be $x$'s true label. The sentence saliency for sentence $s_k$ is:\n",
    "\n",
    "$$S(y|s_k) = P(y|x) - P(x|s_1s_2\\dots s_{k-1}s_{k+1}\\dots s_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_saliency(model, sentences, label):\n",
    "    true_pred = predict_str(model, ' '.join(sentences))\n",
    "    if label[0] == 1:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "    scores = []\n",
    "    for i in range(len(sentences)):\n",
    "        x_hat = ' '.join(sentences[0:i] + sentences[i+1:])\n",
    "        scores.append(true_pred[idx] - predict_str(model, x_hat)[idx])\n",
    "    \n",
    "    # normalize w/ softmax\n",
    "    determinism = 2.0\n",
    "    scores = np.array(scores)\n",
    "    softmax = np.exp(determinism * scores)\n",
    "    softmax /= np.sum(softmax)\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_scores = sentence_saliency(model, sentences, yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12932211 0.12282395 0.11874726 0.12187595 0.1260943  0.1195102\n",
      " 0.11857406 0.14305218]\n"
     ]
    }
   ],
   "source": [
    "print(saliency_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence paraphrasing\n",
    "\n",
    "**back translation**: translate input sentence to another language, then translate back to the original language. This is a technique commonly used for evaluation of language translations. Here, we use this technique to quickly generate a rephrase of original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n"
     ]
    }
   ],
   "source": [
    "print(xi_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test google cloud authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/weifanjiang/Documents/Personal/My Project-1e7426894fe6.json\"\n",
    "\n",
    "def implicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "implicit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans (af)\n",
      "Albanian (sq)\n",
      "Amharic (am)\n",
      "Arabic (ar)\n",
      "Armenian (hy)\n",
      "Azerbaijani (az)\n",
      "Basque (eu)\n",
      "Belarusian (be)\n",
      "Bengali (bn)\n",
      "Bosnian (bs)\n",
      "Bulgarian (bg)\n",
      "Catalan (ca)\n",
      "Cebuano (ceb)\n",
      "Chichewa (ny)\n",
      "Chinese (Simplified) (zh)\n",
      "Chinese (Traditional) (zh-TW)\n",
      "Corsican (co)\n",
      "Croatian (hr)\n",
      "Czech (cs)\n",
      "Danish (da)\n",
      "Dutch (nl)\n",
      "English (en)\n",
      "Esperanto (eo)\n",
      "Estonian (et)\n",
      "Filipino (tl)\n",
      "Finnish (fi)\n",
      "French (fr)\n",
      "Frisian (fy)\n",
      "Galician (gl)\n",
      "Georgian (ka)\n",
      "German (de)\n",
      "Greek (el)\n",
      "Gujarati (gu)\n",
      "Haitian Creole (ht)\n",
      "Hausa (ha)\n",
      "Hawaiian (haw)\n",
      "Hebrew (iw)\n",
      "Hindi (hi)\n",
      "Hmong (hmn)\n",
      "Hungarian (hu)\n",
      "Icelandic (is)\n",
      "Igbo (ig)\n",
      "Indonesian (id)\n",
      "Irish (ga)\n",
      "Italian (it)\n",
      "Japanese (ja)\n",
      "Javanese (jw)\n",
      "Kannada (kn)\n",
      "Kazakh (kk)\n",
      "Khmer (km)\n",
      "Kinyarwanda (rw)\n",
      "Korean (ko)\n",
      "Kurdish (Kurmanji) (ku)\n",
      "Kyrgyz (ky)\n",
      "Lao (lo)\n",
      "Latin (la)\n",
      "Latvian (lv)\n",
      "Lithuanian (lt)\n",
      "Luxembourgish (lb)\n",
      "Macedonian (mk)\n",
      "Malagasy (mg)\n",
      "Malay (ms)\n",
      "Malayalam (ml)\n",
      "Maltese (mt)\n",
      "Maori (mi)\n",
      "Marathi (mr)\n",
      "Mongolian (mn)\n",
      "Myanmar (Burmese) (my)\n",
      "Nepali (ne)\n",
      "Norwegian (no)\n",
      "Odia (Oriya) (or)\n",
      "Pashto (ps)\n",
      "Persian (fa)\n",
      "Polish (pl)\n",
      "Portuguese (pt)\n",
      "Punjabi (pa)\n",
      "Romanian (ro)\n",
      "Russian (ru)\n",
      "Samoan (sm)\n",
      "Scots Gaelic (gd)\n",
      "Serbian (sr)\n",
      "Sesotho (st)\n",
      "Shona (sn)\n",
      "Sindhi (sd)\n",
      "Sinhala (si)\n",
      "Slovak (sk)\n",
      "Slovenian (sl)\n",
      "Somali (so)\n",
      "Spanish (es)\n",
      "Sundanese (su)\n",
      "Swahili (sw)\n",
      "Swedish (sv)\n",
      "Tajik (tg)\n",
      "Tamil (ta)\n",
      "Tatar (tt)\n",
      "Telugu (te)\n",
      "Thai (th)\n",
      "Turkish (tr)\n",
      "Turkmen (tk)\n",
      "Ukrainian (uk)\n",
      "Urdu (ur)\n",
      "Uyghur (ug)\n",
      "Uzbek (uz)\n",
      "Vietnamese (vi)\n",
      "Welsh (cy)\n",
      "Xhosa (xh)\n",
      "Yiddish (yi)\n",
      "Yoruba (yo)\n",
      "Zulu (zu)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "translate_client = translate.Client()\n",
    "\n",
    "results = translate_client.get_languages()\n",
    "\n",
    "for language in results:\n",
    "    print(u'{name} ({language})'.format(**language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/weifanjiang/Documents/Personal/My Project-1e7426894fe6.json\"\n",
    "\n",
    "def back_translation(s_in, show_mid=False):\n",
    "    translate_client = translate.Client()\n",
    "    mid_result = translate_client.translate(s_in, target_language=\"ko\")['translatedText']\n",
    "    if show_mid:\n",
    "        print(mid_result.replace(\"&#39;\", \"\\'\"))\n",
    "    en_result = translate_client.translate(mid_result, target_language=\"en\")['translatedText']\n",
    "    return en_result.replace(\"&#39;\", \"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\n",
      "이 바보 같은 영화입니다. 영화관에서 반 이상을 시청하기 전에 관중의 절반 이상이 남았습니다. 나는 쓰라린 끝에 머물렀다. 용기를 나타내려면? 나는 그것을 텔레비전에서 다시 붙 잡았다. 그리고 그것은 훨씬 더 재미 있었다. 여전히 고전적인 일이 아니고 심지어는 항상 재밌지 만 가족이 나에게 자랐습니다. 어쨌든 Jessica Lundy를 좋아합니다. 더 좋은 방법이없고 TV에서 무료로 사용하면 더 나빠질 수 있습니다.\n",
      "This is a silly movie. More than half of the spectators remained before watching more than half at the cinema. I stayed bitter. To show courage? I caught it on television again. And it was much more fun. It's still not classic and it's always fun, but the family grew up for me. Anyway, I like Jessica Lundy. There is no better way and it can get worse if you use it for free on TV.\n"
     ]
    }
   ],
   "source": [
    "print(xi_text)\n",
    "print(back_translation(xi_text, show_mid = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing sentence-level genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`perturb`: randomly select a sentence from the input paragraph with probability propotional to the saliency score of each sentence. Then apply rephrasing to the selected sentence. Return the paragraph with one modified sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(sentences, saliencies):\n",
    "    choice = np.random.choice(sentences, p=saliencies)\n",
    "    rephrase = back_translation(choice)\n",
    "    new_paragraph = [x if (x != choice) else rephrase for x in sentences]\n",
    "    return ' '.join(new_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`genetic`: main function to perform genetic attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic(x0, y0, model, population, generation):\n",
    "    doc = nlp(xi_text)\n",
    "    sentences = sentence_list(doc)\n",
    "    saliency_scores = sentence_saliency(model, sentences, yi)\n",
    "    \n",
    "    gen0 = set()\n",
    "    for i in range(population):\n",
    "        gen0.add(perturb(sentences, saliency_scores))\n",
    "    \n",
    "    print(gen0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it again on television and it was much funnier. It's still not classic and it's always fun, but the family grew up for me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\", \"This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show courage? I caught it again on television and it was much funnier. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\", \"This is a stupid movie. When I saw it in a movie theater more than half the audience left before it was half over. I stayed to the bitter end. To show fortitude? I caught it on television again. And it was much more fun. Still by no means a classic, or even consistently hilarious but the family kinda grew on me. I love Jessica Lundy anyway. If you've nothing better to do and it's free on t.v. you could do worse.\"}\n"
     ]
    }
   ],
   "source": [
    "genetic(xi_text, yi, model, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
